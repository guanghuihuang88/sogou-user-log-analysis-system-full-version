# 数据产生\采集\存储完整流程联调

## ⭐1 应用服务器模拟程序开发

> 现在，我们要实现一个程序，能够从搜狗日志文件`sogoulogs.log`中逐条的读取数据到目标文件中，并控制传输速率

- Java编写数据生成模拟程序`AnalogData.java`

  ```java
  package com.hadoop.hadoop_test;
  
  import java.io.BufferedReader;
  import java.io.BufferedWriter;
  import java.io.FileInputStream;
  import java.io.FileOutputStream;
  import java.io.IOException;
  import java.io.InputStreamReader;
  import java.io.OutputStreamWriter;
  /**
   * 模拟产生数据
   * @author 易水
   *
   */
  public class AnalogData {   
      /**
       * 读取文件数据
       * @param inputFile
       */
      public static void readData(String inputFile, String outputFile) {
          FileInputStream fis = null;
          InputStreamReader isr = null;
          BufferedReader br = null;	
          String tmp = null;
          try {
              fis = new FileInputStream(inputFile);
              isr = new InputStreamReader(fis,"UTF-8");
              br = new BufferedReader(isr);
              //计数器
              int counter=1;
              //按行读取文件数据
              while ((tmp = br.readLine()) != null) {
              	//打印输出读取的数据
                  System.out.println("第"+counter+"行："+tmp);
                  //数据写入文件
                  writeData(outputFile,tmp);
                  counter++;
                  //方便观察效果，控制数据参数速度
                  Thread.sleep(1000);
              }
              isr.close();
          } catch (IOException e) {
              e.printStackTrace();
          } catch (InterruptedException e) {
              e.printStackTrace();
          } finally {
              if (isr != null) {
                  try {
                      isr.close();
                  } catch (IOException e1) {
                  }
              }
          }
      }
      
      /**
       * 文件写入数据
       * @param outputFile
       * @param line
       */
      public static void writeData(String outputFile, String line) {
          BufferedWriter out = null;
          try {
              out = new BufferedWriter(new OutputStreamWriter(
                      new FileOutputStream(outputFile, true)));
              out.write("\n");
              out.write(line);
          } catch (Exception e) {
              e.printStackTrace();
          } finally {
              try {
                  out.close();
              } catch (IOException e) {
                  e.printStackTrace();
              }
          }
      }
      /**
       * 主方法
       * @param args
       */
      public static void main(String args[]){
      	String inputFile = args[0];
      	String outputFile = args[1];
          try {
          	readData(inputFile, outputFile);
          }catch(Exception e){
          }
      }
  }
  ```

- Eclipse 对模拟程序进行打包：`analogData.jar`

- 分别在 hadoop01，hadoop02，hadoop03节点创建脚本目录

  - `mkdir ~/shell`
  - `cd shell`
  - `mkdir bin conf data lib logs`

- 将`analogData.jar`包程序分别上传至 hadoop02、hadoop03 节点`~/shell/lib`目录下

- 在 hadoop02、hadoop03 将`~/data/flume/logs/sogoulogs.log`拷贝到`~/shell/data`目录下

- 启动模拟程序并测试生成数据：`java -cp analogData.jar com.hadoop.java.AnalogData ../data/sogoulogs.log ../data/test.log`

  <img src="https://hexo.oss-cn-beijing.aliyuncs.com/%E9%A1%B9%E7%9B%AE/%E6%90%9C%E7%8B%97%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/107.jpg" alt="image" style="zoom:80%;" />

## ⭐2 应用程序 shell 脚本开发

-  在 hadoop01，hadoop02，hadoop03 的`~/shell/bin`路径下编写通用脚本`common.sh`，并添加可执行权限：`chmod u+x common.sh`

  ```shell
  #!/bin/sh
  #切换到当前目录的父目录
  home=$(cd `dirname $0`; cd ..; pwd)
  bin_home=$home/bin
  conf_home=$home/conf
  logs_home=$home/logs
  lib_home=$home/lib
  
  flume_home=/home/hadoop/app/apache-flume-1.7.0-bin
  kafka_home=/home/hadoop/app/kafka
  
  export HADOOP_HOME=/home/hadoop/app/hadoop
  export HBASE_HOME=/home/hadoop/app/hbase 
  ```

-  在 hadoop02，hadoop03 的`~/shell/bin/`路径下编写模拟程序脚本`sogoulogs.sh`，并添加可执行权限：`chmod u+x sogoulogs.sh`

  ```shell
  #!/bin/sh
  home=$(cd `dirname $0`; cd ..; pwd)
  . ${home}/bin/common.sh
  echo "start analog data ****************"
  java -cp ${lib_home}/analogData.jar com.hadoop.java.AnalogData  ${data_home}/sogoulogs.log ${data_home}/test.log
  ```

- 在 hadoop02，hadoop03 执行脚本：`bin/sogoulogs.sh`，可以看到，第一节开发的应用程序的数据传输功能成功由脚本实现

  <img src="https://hexo.oss-cn-beijing.aliyuncs.com/%E9%A1%B9%E7%9B%AE/%E6%90%9C%E7%8B%97%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/108.jpg" alt="image" style="zoom:80%;" />

## 3 flume shell 脚本开发

分别在 hadoop01，hadoop02，hadoop03 的`~/shell/bin`路径下编写 flume 启动脚本，并将`flume-kafka-hbase.properties`和`flume-conf.properties`配置文件分别放到 hadoop01，hadoop02，hadoop03 的`~/shell/conf`路径下

- `start_flume_hadoop01.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  
  home=$(cd `dirname $0`; cd ..; pwd)
  
  . ${home}/bin/common.sh
  
  ${flume_home}/bin/flume-ng agent \
  --conf ${conf_home} \
  -f ${conf_home}/flume-kafka-hbase.properties -n agent >> ${logs_home}/flume-hadoop01.log 2>&1 &
  
  echo $! > ${logs_home}/flume-hadoop01.pid
  ```

- `start_flume_hadoop02.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  
  home=$(cd `dirname $0`; cd ..; pwd)
  
  . ${home}/bin/common.sh
  
  ${flume_home}/bin/flume-ng agent \
  --conf ${conf_home} \
  -f ${conf_home}/flume-conf.properties -n agent >> ${logs_home}/flume-hadoop02.log 2>&1 &
  
  echo $! > ${logs_home}/flume-hadoop02.pid
  ```

- `start_flume_hadoop03.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  
  home=$(cd `dirname $0`; cd ..; pwd)
  
  . ${home}/bin/common.sh
  
  ${flume_home}/bin/flume-ng agent \
  --conf ${conf_home} \
  -f ${conf_home}/flume-conf.properties -n agent >> ${logs_home}/flume-hadoop03.log 2>&1 &
  
  echo $! > ${logs_home}/flume-hadoop03.pid
  ```

分别在 hadoop01，hadoop02，hadoop03 的`~/shell/bin`路径下编写 flume 关闭脚本

- `stop_flume_hadoop01.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  
  home=$(cd `dirname $0`; cd ..; pwd)
  
  . ${home}/bin/common.sh
  
  pid=`cat ${logs_home}/flume-hadoop01.pid | head -1`
  
  kill ${pid}
  ```

- `stop_flume_hadoop02.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  
  home=$(cd `dirname $0`; cd ..; pwd)
  
  . ${home}/bin/common.sh
  
  pid=`cat ${logs_home}/flume-hadoop02.pid | head -1`
  
  kill ${pid}
  ```

- `stop_flume_hadoop03.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  
  home=$(cd `dirname $0`; cd ..; pwd)
  
  . ${home}/bin/common.sh
  
  pid=`cat ${logs_home}/flume-hadoop03.pid | head -1`
  
  kill ${pid}
  ```

## 4 kafka 消费者 shell

分别在 hadoop01，hadoop02，hadoop03 的`~/shell/bin`路径下编写 kafka 消费者脚本：

- `start_kafka_consumer.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  home=$(cd `dirname $0`; cd ..; pwd)
  . ${home}/bin/common.sh
  echo "start kafka consumer *****************"
  ${kafka_home}/bin/kafka-console-consumer.sh  --zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka1.0  --topic sogoulogs
  ```

- `start_kafka_consumer.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  home=$(cd `dirname $0`; cd ..; pwd)
  . ${home}/bin/common.sh
  echo "start kafka consumer *****************"
  ${kafka_home}/bin/kafka-console-consumer.sh  --zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka1.0  --topic sogoulogs
  ```

- `start_kafka_consumer.sh`：并添加可执行权限`chmod u+x`

  ```shell
  #!/bin/sh
  home=$(cd `dirname $0`; cd ..; pwd)
  . ${home}/bin/common.sh
  echo "start kafka consumer *****************"
  ${kafka_home}/bin/kafka-console-consumer.sh  --zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka1.0  --topic sogoulogs
  ```

## 5 启动集群相关服务

- 启动 Zookeeper：`runRemoteCmd.sh "/home/hadoop/app/zookeeper/bin/zkServer.sh start" all`

- 注意集群时钟必须同步：

  同步时间：`sudo ntpdate pool.ntp.org`
  查看时间：`date`

- 启动 HDFS：`sbin/start-dfs.sh`

- 在 hadoop01 启动 HBase：`bin/start-hbase.sh`

  进入`bin/hbase shell`

- 每个节点分别启动 Kafka 集群：`bin/kafka-server-start.sh config/server.properties`

## 6 准备工作

- 在 hadoop01，hadoop02，hadoop03 将`sogoulogs.log`和空的`test.log`文件数据放到`~/shell/data/`目录下
- 修改 hadoop01，hadoop02，hadoop03 中`~/shell/conf`中的 flume 配置文件中数据路径为
  - `/home/hadoop/shell/data/sogoulogs.log`
  - `/home/hadoop/shell/data/test.log`

## 7 数据产生,采集,存储联调

- 在 hadoop01 启动 kafka 消费者：`bin/start_kafka_consumer.sh`

- 在 hadoop01 启动 flume 监听数据：`bin/start_flume_hadoop01.sh`

- 在 hadoop02 启动 flume 采集数据：`bin/start_flume_hadoop02.sh`

- 在 hadoop03 启动 flume 采集数据：`bin/start_flume_hadoop03.sh`

- 在 hadoop02 启动模拟程序脚本产生数据：`bin/sogoulogs.sh`

  <img src="https://hexo.oss-cn-beijing.aliyuncs.com/%E9%A1%B9%E7%9B%AE/%E6%90%9C%E7%8B%97%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/109.jpg" alt="image" style="zoom:80%;" />

- 于是，可以在 HBase 中查看到数据已被存储，kafka 消费者也消费了这些数据

  <img src="https://hexo.oss-cn-beijing.aliyuncs.com/%E9%A1%B9%E7%9B%AE/%E6%90%9C%E7%8B%97%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/110.jpg" alt="image" style="zoom:80%;" />

  <img src="https://hexo.oss-cn-beijing.aliyuncs.com/%E9%A1%B9%E7%9B%AE/%E6%90%9C%E7%8B%97%E7%94%A8%E6%88%B7%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/111.jpg" alt="image" style="zoom:80%;" />



